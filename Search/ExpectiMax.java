package Search;

import Environment.BitBoard;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;

/**
 * Created by Rob on 11/6/2016.
 */
public class ExpectiMax {

    /**
     * The Maximum depth of the search tree to be generated
     */
    private int MAX_DEPTH = 8;

    /**
     * We will not consider any state in which there is a lower probability
     * of reaching than this minimum. This allows us to filter out a large number of
     * states which are hardly worth considering in our final descision. (i.e. getting five "4" tiles
     * placed in a row)
     */
    private float MINIMUM_STATE_PROBABILITY = .001f;

    /**
     * The cache is used to store the values of previously evaluated nodes,
     * this allows us to quickly evaluate states that appear more than once
     * in a single descision tree. This massivly reduces the branhcing factor when
     * low depth nodes get a cache hit.
     */
    private HashMap<Long, Double> CACHE = new HashMap<>();
    private int CACHE_DEPTH = 12;

    /**
     * Bitboard representation of a single "2" tile, view Bitboard.java for a detailed explanation.
     * This value will be used to place tiles in the probabilistic levels of the tree
     */
    private long  TILE_TWO = 0x1L;


    /**
     * Decides the best possible action according to our heuristics and search depth
     * Does not record any data (outside of a copy of the cache until the next search is ran)
     *
     * @param state Current game state
     * @return The state resulting from the best action
     */
    public long decideAction( long state ) {
        long bestMove = 0L;
        double bestMoveValue = -1.0D / 0.0;

        /**
         * For optimization purposes, the moves generated by MoveGenerator.generateMoves()
         * are actually the resulting game state of each possible move, as there is no
         * need to record the actual move
         */
        Iterator moveIterator = MoveGenerator.generateMoves( state ).iterator();

        while( moveIterator.hasNext() ) {
            long action =  (long)moveIterator.next();
            /**
             * For probvalue(action, 0, 1.0f)
             * The 0 is the initial depth (0 since we are at the root)
             * The initial probability is 1.0f as there is a 100% chance
             * of the game being in this state.
             */
            double currentValue = this.probValue( action , 0 , 1.0f );
            if( currentValue > bestMoveValue ) {
                bestMove = action;
                bestMoveValue = currentValue;
            }
        }
        return bestMove;

    }

    /**
     * Standard Maximization stage of ExpecitMax search. Finds the move with the best value from
     * the previous depths probabilistic stage
     *
     * @param state Current state we are trying to maximize
     * @param depth The current depth of the search tree
     * @param cprob The current cutoff probability of this state
     * @return returns the value of the best move evaluated in the subtree
     */
    public double maxValue( long state , int depth , float cprob ) {

        if( cprob < MINIMUM_STATE_PROBABILITY || depth == MAX_DEPTH ) {
            return Evaluator.evaluateBoard( state );
        }

        if( CACHE.containsKey( state ) ){
            return CACHE.get( state );
        }

        if( BitBoard.isTerminal( state ) ) {
            return 0;
        }

        double maxValue = 0D;
        long action = 0L;
        List<Long> moves = MoveGenerator.generateMoves( state );
        int len = moves.size();
        for( int i = 0; i < len; i++ ) {
            action = moves.get( i );
            maxValue = Math.max( maxValue , this.probValue( action , depth+1 , cprob ) );
        }
        //if(CACHE_DEPTH >= depth) {
            CACHE.put(state, maxValue);
        //}
        return maxValue;

    }

    /**
     * Probabilistic stage of Expecimax Search, determines the sum of all
     * probable tile placements multiplied by their actual probability
     *
     * @param state current state that we are adding a tile to
     * @param depth current search depth
     * @param cprob current cutoff probability of this state
     * @return
     */
    public double probValue( long state , int depth , float cprob ) {
        if( depth == MAX_DEPTH ){
            return Evaluator.evaluateBoard( state );
        }
        double value = 0;
        /**
         * Here we divide cprob by the number of empty spaces, as each tile has a 1/emptySpaces chance
         * of appearing in a specific space
         */
        double emptySpaces = BitBoard.emptySpaces( state );
        cprob /= emptySpaces;
        long mask = 0x000000000000000FL; //used to extract each 4 bit sqaure on the board
        /**
         * For the actual probabilistic algoritm, we start by iterating over all 16 square.
         * if the bitwise AND of the mask and state is 0, we know this square is empty and that a tile may
         * be placed. Each space has a 90% chance of being a TWO and a 10% chance of being a 4. So we add
         * the value of each possible outcome multiplied by the probability of it ocurring.
         * NOTE: The FOUR tile is generated by bitshifting the TWO tile 1 bit left
         * We then return the average of these values by dividing by the number of empty spaces.
         */
        for( int i = 0; i < 16; i++ ) {
            if( ( mask & state ) == 0 ) {
                value += .9 * ( maxValue( (state | (   TILE_TWO        << ( i * 4 ) ) ), depth + 1, cprob * .9f ) );
                value += .1 * ( maxValue( (state | ( ( TILE_TWO << 1 ) << ( i * 4 ) ) ), depth + 1, cprob * .1f ) );
            }
            mask = mask << 4;
        }
        return value / emptySpaces;
    }


    public int getDepth() { return MAX_DEPTH; }

    public void setDepth(int depth) { this.MAX_DEPTH = depth; }
    public void setFilterProbability(float prob) { this.MINIMUM_STATE_PROBABILITY = prob;}


}